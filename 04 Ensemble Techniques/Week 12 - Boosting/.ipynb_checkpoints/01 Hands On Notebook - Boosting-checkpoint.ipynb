{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: \n",
    "\n",
    "This notebook is an updated version of the notebook used in the **hands-on lecture**.\n",
    "\n",
    "The following changes have been made:\n",
    "\n",
    "1. random_state parameter is set in the Decision Tree model used as base_estimator while tuning the AdaBoostClassifier().\n",
    "\n",
    "    * In the video, the random_state parameter was not set in the Decision Tree model while tuning AdaBoostClassifier, which might result in different hyperparameters being selected by the GridSearchCV(). In the updated notebook, we have set the random_state value, which will ensure that results do not change when the code cell is re-run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RB92HxSfxLPj"
   },
   "source": [
    "# Ensemble Hands On - Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fy_ZAgmzxLPk"
   },
   "source": [
    "We are going to build a model that predicts if someone who seeks a loan might be a defaulter or a non-defaulter. We have several independent variables like, checking account balance, credit history, purpose, loan amount etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFyI3CAtxLPk"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8NjHSaCTxLPl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "#To install xgboost library use - !pip install xgboost \n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will use the same `credit data` used in decision tree hands-on lecture.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vgm9EEt5xLPo",
    "outputId": "3120b7c3-2035-4c20-9107-5db2ac686ca1",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'credit.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-15c6d7420ef9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreditData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"credit.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#creditData = pd.read_csv(\"credit.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcreditData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#several missing values!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'credit.csv'"
     ]
    }
   ],
   "source": [
    "creditData = pd.read_csv(\"./datasets/credit.csv\")\n",
    "\n",
    "#creditData = pd.read_csv(\"credit.csv\")\n",
    "creditData.head(10) #several missing values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4t1OBSqzxLPr",
    "outputId": "c5db5b2b-a257-4d03-b2cf-ef59e8bed4a2"
   },
   "outputs": [],
   "source": [
    "creditData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditData['default'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32heA0VFxLPt",
    "outputId": "f1119e0b-fa26-43ea-ba6c-f6d77ae2354c"
   },
   "outputs": [],
   "source": [
    "creditData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uv_-CH_JxLPx",
    "outputId": "36a727e2-731c-4dd1-cb3f-53192e55bd1f"
   },
   "outputs": [],
   "source": [
    "creditData.info()  # many columns are of type object i.e. strings. These need to be converted to ordinal type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVDy-BlVxLPz"
   },
   "source": [
    "Lets convert the columns with an 'object' datatype into categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQVE5XZJxLPz",
    "outputId": "683bab0e-8751-42e5-85e0-b48d80399907"
   },
   "outputs": [],
   "source": [
    "for feature in creditData.columns: # Loop through all columns in the dataframe\n",
    "    if creditData[feature].dtype == 'object': # Only apply for columns with categorical strings\n",
    "        creditData[feature] = pd.Categorical(creditData[feature])# Replace strings with an integer\n",
    "creditData.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jf4JAhwJxLP2",
    "outputId": "4719ccee-2e82-4174-e740-ebd0d7166d2d"
   },
   "outputs": [],
   "source": [
    "print(creditData.checking_balance.value_counts())\n",
    "print(creditData.credit_history.value_counts())\n",
    "print(creditData.purpose.value_counts())\n",
    "print(creditData.savings_balance.value_counts())\n",
    "print(creditData.employment_duration.value_counts())\n",
    "print(creditData.other_credit.value_counts())\n",
    "print(creditData.housing.value_counts())\n",
    "print(creditData.job.value_counts())\n",
    "print(creditData.phone.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tgft_TIVxLP4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "replaceStruct = {\n",
    "                \"checking_balance\":     {\"< 0 DM\": 1, \"1 - 200 DM\": 2 ,\"> 200 DM\": 3 ,\"unknown\":-1},\n",
    "                \"credit_history\": {\"critical\": 1, \"poor\":2 , \"good\": 3, \"very good\": 4,\"perfect\": 5},\n",
    "                 \"savings_balance\": {\"< 100 DM\": 1, \"100 - 500 DM\":2 , \"500 - 1000 DM\": 3, \"> 1000 DM\": 4,\"unknown\": -1},\n",
    "                 \"employment_duration\":     {\"unemployed\": 1, \"< 1 year\": 2 ,\"1 - 4 years\": 3 ,\"4 - 7 years\": 4 ,\"> 7 years\": 5},\n",
    "                \"phone\":     {\"no\": 1, \"yes\": 2 },\n",
    "                #\"job\":     {\"unemployed\": 1, \"unskilled\": 2, \"skilled\": 3, \"management\": 4 },\n",
    "                \"default\":     {\"no\": 0, \"yes\": 1 } \n",
    "                    }\n",
    "oneHotCols=[\"purpose\",\"housing\",\"other_credit\",\"job\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxeyJbJFxLP6",
    "outputId": "ec5bb8e3-b09d-4920-f84b-d85791f37ff4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "creditData=creditData.replace(replaceStruct)\n",
    "creditData=pd.get_dummies(creditData, columns=oneHotCols)\n",
    "creditData.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqTiaxCrxLP8",
    "outputId": "ffb94d79-da96-4923-937a-7171b38eec97"
   },
   "outputs": [],
   "source": [
    "creditData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0L-oAMItxLP-"
   },
   "source": [
    "## Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When data (classification) exhibit a significant imbalance in the distribution of the target classes, it is good to use stratified sampling to ensure that relative class frequencies are approximately preserved in train and test sets. \n",
    "- This is done by setting the `stratify` parameter to target variable in the train_test_split function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWq265BvxLP_"
   },
   "outputs": [],
   "source": [
    "X = creditData.drop(\"default\" , axis=1)\n",
    "y = creditData.pop(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8EIKHRCmxLQB"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=1,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will use the same functions, created in bagging hands-on lecture, to calculate different metrics and plot the confusion matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create confusion matrix\n",
    "def make_confusion_matrix(model,y_actual,labels=[1, 0]):\n",
    "    '''\n",
    "    model : classifier to predict values of X\n",
    "    y_actual : ground truth  \n",
    "    \n",
    "    '''\n",
    "    y_predict = model.predict(X_test)\n",
    "    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[0, 1])\n",
    "    df_cm = pd.DataFrame(cm, index = [i for i in [\"Actual - No\",\"Actual - Yes\"]],\n",
    "                  columns = [i for i in ['Predicted - No','Predicted - Yes']])\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         cm.flatten()/np.sum(cm)]\n",
    "    labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n",
    "              zip(group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.heatmap(df_cm, annot=labels,fmt='')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Function to calculate different metric scores of the model - Accuracy, Recall and Precision\n",
    "def get_metrics_score(model,flag=True):\n",
    "    '''\n",
    "    model : classifier to predict values of X\n",
    "\n",
    "    '''\n",
    "    # defining an empty list to store train and test results\n",
    "    score_list=[] \n",
    "    \n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    \n",
    "    train_acc = model.score(X_train,y_train)\n",
    "    test_acc = model.score(X_test,y_test)\n",
    "    \n",
    "    train_recall = metrics.recall_score(y_train,pred_train)\n",
    "    test_recall = metrics.recall_score(y_test,pred_test)\n",
    "    \n",
    "    train_precision = metrics.precision_score(y_train,pred_train)\n",
    "    test_precision = metrics.precision_score(y_test,pred_test)\n",
    "    \n",
    "    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision))\n",
    "        \n",
    "    # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.\n",
    "    if flag == True: \n",
    "        print(\"Accuracy on training set : \",model.score(X_train,y_train))\n",
    "        print(\"Accuracy on test set : \",model.score(X_test,y_test))\n",
    "        print(\"Recall on training set : \",metrics.recall_score(y_train,pred_train))\n",
    "        print(\"Recall on test set : \",metrics.recall_score(y_test,pred_test))\n",
    "        print(\"Precision on training set : \",metrics.precision_score(y_train,pred_train))\n",
    "        print(\"Precision on test set : \",metrics.precision_score(y_test,pred_test))\n",
    "    \n",
    "    return score_list # returning the list with train and test scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "- We are going to build 3 ensemble models here - AdaBoost Classifier, Gradient Boosting Classifier and XGBoost Classifier.\n",
    "- First, let's build these models with default parameters and then use hyperparameter tuning to optimize the model performance.\n",
    "- We will calculate all three metrics - Accuracy, Precision and Recall but the metric of interest here is recall.\n",
    "- `Recall` - It gives the ratio of True positives to Actual positives, so high Recall implies low false negatives, i.e. low chances of predicting a defaulter as non defaulter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier(random_state=1)\n",
    "abc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using above defined function to get accuracy, recall and precision on train and test set\n",
    "abc_score=get_metrics_score(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(abc,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(random_state=1)\n",
    "gbc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using above defined function to get accuracy, recall and precision on train and test set\n",
    "gbc_score=get_metrics_score(gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(gbc,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(random_state=1,eval_metric='logloss')\n",
    "xgb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using above defined function to get accuracy, recall and precision on train and test set\n",
    "xgb_score=get_metrics_score(xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(xgb,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With default parameters:**\n",
    "- AdaBoost classifier has better test accuracy among these 3 models.\n",
    "- GB classifier has least test accuracy and test recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\n",
    "- Some important hyperparamters are:\n",
    "    - base_estimator: The base estimator from which the boosted ensemble is built. By default the base estimator is a decision tree with max_depth=1\n",
    "    - n_estimators: The maximum number of estimators at which boosting is terminated. Default value is 50.\n",
    "    - learning_rate: Learning rate shrinks the contribution of each classifier by learning_rate. There is a trade-off between learning_rate and n_estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the type of classifier. \n",
    "abc_tuned = AdaBoostClassifier(random_state=1)\n",
    "\n",
    "# Grid of parameters to choose from\n",
    "## add from article\n",
    "parameters = {\n",
    "    #Let's try different max_depth for base_estimator\n",
    "    \"base_estimator\":[DecisionTreeClassifier(max_depth=1, random_state=1),DecisionTreeClassifier(max_depth=2, random_state=1),DecisionTreeClassifier(max_depth=3, random_state=1)],\n",
    "    \"n_estimators\": np.arange(10,110,10),\n",
    "    \"learning_rate\":np.arange(0.1,2,0.1)\n",
    "}\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "acc_scorer = metrics.make_scorer(metrics.recall_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(abc_tuned, parameters, scoring=acc_scorer,cv=5)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "abc_tuned = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data.\n",
    "abc_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using above defined function to get accuracy, recall and precision on train and test set\n",
    "abc_tuned_score=get_metrics_score(abc_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(abc_tuned,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "- The model is overfitting the train data as train accuracy is much higher than the test accuracy.\n",
    "- The model has low test recall. This implies that the model is not good at identifying defaulters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = abc_tuned.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "feature_names = list(X.columns)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='violet', align='center')\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Amount is the most important feature as per the tuned AdaBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most of the hyperparameters available are same as random forest classifier.\n",
    "- init: An estimator object that is used to compute the initial predictions. If ‘zero’, the initial raw predictions are set to zero. By default, a DummyEstimator predicting the classes priors is used.\n",
    "- There is no class_weights parameter in gradient boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try using AdaBoost classifier as the estimator for initial predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_init = GradientBoostingClassifier(init=AdaBoostClassifier(random_state=1),random_state=1)\n",
    "gbc_init.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using above defined function to get accuracy, recall and precision on train and test set\n",
    "gbc_init_score=get_metrics_score(gbc_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As compared to the model with default parameters:**\n",
    "\n",
    "- Test accuracy and test recall have increased slightly.\n",
    "- As we are getting better results, we will use init = AdaBoostClassifier() to tune the gradient boosting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choose the type of classifier. \n",
    "gbc_tuned = GradientBoostingClassifier(init=AdaBoostClassifier(random_state=1),random_state=1)\n",
    "\n",
    "# Grid of parameters to choose from\n",
    "## add from article\n",
    "parameters = {\n",
    "    \"n_estimators\": [100,150,200,250],\n",
    "    \"subsample\":[0.8,0.9,1],\n",
    "    \"max_features\":[0.7,0.8,0.9,1]\n",
    "}\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "acc_scorer = metrics.make_scorer(metrics.recall_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(gbc_tuned, parameters, scoring=acc_scorer,cv=5)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "gbc_tuned = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data.\n",
    "gbc_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using above defined function to get accuracy, recall and precision on train and test set\n",
    "gbc_tuned_score=get_metrics_score(gbc_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(gbc_tuned,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "- The model performace has not increased by much.\n",
    "- The model has started to overfit the train data in terms of recall.\n",
    "- It is better at identifying non-defaulters than identifying defaulters which is the opposite of the result we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = gbc_tuned.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "feature_names = list(X.columns)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='violet', align='center')\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Amount is the most important feature, followed by loan duration and checking balance, as per the tuned gradient boosting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost has many hyper parameters which can be tuned to increase the model performance. You can read about them in the xgboost documentation [here](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters). Some of the important parameters are:**\n",
    "- scale_pos_weight:Control the balance of positive and negative weights, useful for unbalanced classes. It has range from 0 to $\\infty$.\n",
    "- subsample: Corresponds to the fraction of observations (the rows) to subsample at each step. By default it is set to 1 meaning that we use all rows.\n",
    "- colsample_bytree: Corresponds to the fraction of features (the columns) to use.\n",
    "- colsample_bylevel: The subsample ratio of columns for each level. Columns are subsampled from the set of columns chosen for the current tree.\n",
    "- colsample_bynode: The subsample ratio of columns for each node (split). Columns are subsampled from the set of columns chosen for the current level.\n",
    "- max_depth: is the maximum number of nodes allowed from the root to the farthest leaf of a tree.\n",
    "- learning_rate/eta: Makes the model more robust by shrinking the weights on each step.\n",
    "- gamma: A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the type of classifier. \n",
    "xgb_tuned = XGBClassifier(random_state=1,eval_metric='logloss')\n",
    "\n",
    "# Grid of parameters to choose from\n",
    "## add from\n",
    "parameters = {\n",
    "    \"n_estimators\": np.arange(10,100,20),\n",
    "    \"scale_pos_weight\":[0,1,2,5],\n",
    "    \"subsample\":[0.5,0.7,0.9,1],\n",
    "    \"learning_rate\":[0.01,0.1,0.2,0.05],\n",
    "    \"gamma\":[0,1,3],\n",
    "    \"colsample_bytree\":[0.5,0.7,0.9,1],\n",
    "    \"colsample_bylevel\":[0.5,0.7,0.9,1]\n",
    "}\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "acc_scorer = metrics.make_scorer(metrics.recall_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(xgb_tuned, parameters,scoring=acc_scorer,cv=5)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "xgb_tuned = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data.\n",
    "xgb_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using above defined function to get accuracy, recall and precision on train and test set\n",
    "xgb_tuned_score=get_metrics_score(xgb_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(xgb_tuned,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "- The test accuracy of the model has reduced as compared to the model with default parameters but the recall has increased significantly and the model is able to identify most of the defaulters.\n",
    "- Decreasing number of false negatives has increased the number of false positives here.\n",
    "- The tuned model is not overfitting and generalizes well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = xgb_tuned.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "feature_names = list(X.columns)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='violet', align='center')\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checking balance is the most important feature as per XGBoost model unlike AdaBoost and Gradient Boosting, where the most important feature is the amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining list of models\n",
    "models = [abc, abc_tuned, gbc, gbc_init, gbc_tuned, xgb, xgb_tuned]\n",
    "\n",
    "# defining empty lists to add train and test results\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "recall_train = []\n",
    "recall_test = []\n",
    "precision_train = []\n",
    "precision_test = []\n",
    "\n",
    "# looping through all the models to get the accuracy, precall and precision scores\n",
    "for model in models:\n",
    "    j = get_metrics_score(model,False)\n",
    "    acc_train.append(np.round(j[0],2))\n",
    "    acc_test.append(np.round(j[1],2))\n",
    "    recall_train.append(np.round(j[2],2))\n",
    "    recall_test.append(np.round(j[3],2))\n",
    "    precision_train.append(np.round(j[4],2))\n",
    "    precision_test.append(np.round(j[5],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_frame = pd.DataFrame({'Model':['AdaBoost with default paramters','AdaBoost Tuned', \n",
    "                                          'Gradient Boosting with default parameters','Gradient Boosting with init=AdaBoost',\n",
    "                                          'Gradient Boosting Tuned','XGBoost with default parameters','XGBoost Tuned'], \n",
    "                                          'Train_Accuracy': acc_train,'Test_Accuracy': acc_test,\n",
    "                                          'Train_Recall':recall_train,'Test_Recall':recall_test,\n",
    "                                          'Train_Precision':precision_train,'Test_Precision':precision_test}) \n",
    "comparison_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark:\n",
    "- A `Cost Function` quantifies the error between predicted values and expected values and presents it in the form of a single real number.\n",
    "- The bank can choose a model depending on their cost function which they want to minimize. The cost function can depend on the geographical and economical conditions due to changes in interest rates, government rules etc.\n",
    "- Bank's main aim would be to balance the trade off between losing an opportunity (to gain money by giving loans) in case of FP and losing the money in case of FN.\n",
    "- We emphasized that recall is the metric of interest here and we tuned our model on recall. But this does not mean that other metrics should be ignored completely.\n",
    "- Here, we assumed that the $cost\\ on\\ FN>cost\\ on\\ FP$, but we do not want to misclassify so many non-defaulters that the equation is reversed i.e. $cost\\ on\\ FP>cost\\ on\\ FN$, hence, the bank will actually be losing money in the longer run.\n",
    "- `For example:` Let's say a bank gains 4% interest on the amount while giving loans to non-defaulters and lose 70% on the amount while giving loans to defaulters and also assume that the amount of loan is also fixed. Let's say we use a model which is only good at identifying defaulters and we get the following result:\n",
    "    - The model identifies 10 out of 10 defaulters\n",
    "    - The model identifies 10 out of 190 non-defaulters i.e. misclassified 180 non-defaulters\n",
    "    - Then, the money saved is $0.7*10*amount=7*amount$ and the money lost is $0.04*180*amount=7.2*amount$\n",
    "    - As the value of money lost is greater than the value of money saved, bank will actually lose money even after identifying all defaulters"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Decision_Tree_Notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
